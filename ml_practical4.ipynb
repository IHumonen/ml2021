{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the documentation for scipy.optimize.minimize, paying special attention to the Jacobian argument jac. Who computes the gradient, the minimize function itself, or the developer using it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The developer computes the gradient through argument jac if it is callable: \"If it is a callable, it should be a function that returns the gradient vector: jac(x, *args) -> array_like, shape (n,)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following two examples; which performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize, numpy.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: 0.0\n",
      " hess_inv: array([[0.5]])\n",
      "      jac: array([0.])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 7\n",
      "      nit: 3\n",
      "     njev: 7\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([0.])\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "  return x**2\n",
    "\n",
    "def df(x):\n",
    "  return 2*x\n",
    "\n",
    "print(scipy.optimize.minimize(f, numpy.random.randint(-1000, 1000), jac=df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: 6.566365205259904e-17\n",
      " hess_inv: array([[0.49999946]])\n",
      "      jac: array([-1.30546116e-09])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 24\n",
      "      nit: 4\n",
      "     njev: 8\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([-8.10331118e-09])\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "  return x**2\n",
    "\n",
    "print(scipy.optimize.minimize(f, numpy.random.randint(-1000, 1000), jac=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first example performs better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Write in python the loss function for support vector machines from equation (7.48) of Daum√©. You can use the following hinge loss surrogate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss_surrogate(y_gold, y_pred):\n",
    "  return numpy.max(0, 1 - y_gold * y_pred)\n",
    "\n",
    "def svm_loss(w, b, C, D):\n",
    "    #D = [x, y]\n",
    "    x = D[0]\n",
    "    y_gold = D[1]\n",
    "    y_pred = numpy.dot(w*x) + b\n",
    "    l = 0.5*numpy.norm(w)**2 + C*numpy.sum(hinge_loss_surrogate(y_gold, y_pred))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Use scipy.optimize.minimize with jac=False to implement support vector machines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(D):\n",
    "    # compute w and b with scipy.optimize.minimize and return them\n",
    "    result = scipy.optimize.minimize(lambda x: svm_loss(x[:-1], x[-1], 1, D), numpy.random.randint(-10, 10), jac=False)\n",
    "    return result['x'][:-1], result['x'][-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "Implement the gradient of svm_loss, and add an optional flag to svm to use it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient of $f$ = svm_loss:   \n",
    "        \n",
    "$\\frac{\\partial \\xi_n}{\\partial w} = 0 $ or $ -y_nx_n$      \n",
    "     \n",
    "$\\frac{\\partial \\xi_n}{\\partial b} = 0 $ or $ -y_n$      \n",
    "        \n",
    "$\\frac{\\partial f}{\\partial w} = w + C \\sum\\limits_{n}{(\\frac{\\partial \\xi_n}{\\partial w}x_n)}$    \n",
    "     \n",
    "$\\frac{\\partial f}{\\partial b} = C \\sum\\limits_{n}{\\frac{\\partial \\xi_n}{\\partial b}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_hinge_loss_surrogate(y_gold, y_pred):\n",
    "  if hinge_loss_surrogate(y_gold, y_pred) == 0:\n",
    "    return [0, 0]\n",
    "  else:\n",
    "    return [-y_pred, -y_gold]\n",
    "\n",
    "def gradient_hinge_loss_surrogate(y_gold, x, w, b):\n",
    "  if hinge_loss_surrogate(y_gold, y_pred) == 0:\n",
    "    return [0, 0]\n",
    "  else:\n",
    "    return [-y_gold*x, -y_gold]\n",
    "\n",
    "def gradient_svm_loss(w, b, C, D):\n",
    "    #D = [x, y]\n",
    "    x = D[0]\n",
    "    y_gold = D[1]\n",
    "    y_pred = numpy.dot(w*x) + b\n",
    "    l_w = w + C*numpy.sum(gradient_hinge_loss_surrogate(y_gold, x, w, b))\n",
    "    l_b = C*numpy.sum(gradient_hinge_loss_surrogate(y_gold, x, w, b))\n",
    "    return numpy.concatenate((l_w, l_b))\n",
    "\n",
    "def svm(D, use_gradient=False):\n",
    "    if use_gradient != False:\n",
    "        use_gradient = lambda x: gradient_svm_loss(x[:-1], x[-1], 1, D)\n",
    "    result = scipy.optimize.minimize(lambda x: svm_loss(x[:-1], x[-1], 1, D), numpy.random.randint(-10, 10), jac=use_gradient)\n",
    "    return result['x'][:-1], result['x'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
